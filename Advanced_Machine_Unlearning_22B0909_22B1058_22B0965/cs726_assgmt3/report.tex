\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{enumitem}
\usepackage[a4paper,margin=1in]{geometry}
\setlength{\parindent}{0pt}
\setlength{\parskip}{4pt}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

\title{Programming Assignment 3: Advanced Machine Learning}
\author{
    Advanced Machine Unlearning \\
    Tejas Sharma: 22B0909 \\
    Muskaan Jain: 22B1058 \\
    Ojas Maheshwari: 22B0965
}
\date{}

\begin{document}

\maketitle
\tableofcontents
\clearpage


\section{Task 0: LLM Decoding Techniques}

\subsection{Algorithms:}

\subsubsection{Greedy Decoding:} At every step, we use the logits to determine the probability for each token, and choose the one with the highest probability. We do not need to compute the exact probabilities using softmax, as the logit values are only scaled, and so, the max probability token is the token corresponding to the highest logit value.\\
We append the generated token to the input sequence. When the EOS token is generated, we break the loop.

\subsubsection{Random Sampling:}
Here, we first apply softmax to the logits to compute probabilities with temperature = 1. \\
Then we raise all the probabilities to the power $1/\tau$, followed by normalization to obtain the probability distribution while adjusting its sharpness using the temperature parameter $\tau$. \\
Then we sample one value from this distribution using \textit{torch.multinomial}. \\
Similar to the previous part, we append the generated token to the input sequence. When the EOS token is generated, we break the loop.

\subsubsection{Top-k Sampling:}
Here, we first apply softmax to the logits to compute probabilities with temperature = 1. \\
Then we obtain the top-k probabilities and their corresponding indices, followed by normalization of these k probability values.\\
Similar to the previous part, we sample one value from this distribution and append the generated token to the input sequence. When the EOS token is generated, we break the loop.

\subsubsection{Nucleus Sampling:}
Here, we first apply softmax to the logits to compute probabilities with temperature = 1. \\
Then we sort the probability values (in decreasing order) and compute the cumulative probabilities. \\
Then, we choose the index at which the cumulative probability exceeds the nucleus p, and normalize the (non-cumulative) probabilities of all the tokens that contribute to the cumulative probability at this index. We ensure that we always select at least one token as follows: If no tokens have been selected, select the highest probability (greedy) token. \\
Similar to the previous part, we sample one value from this distribution and append the generated token to the input sequence. When the EOS token is generated, we break the loop.

\subsection{BLEU and ROUGE Metrics:}
\subsubsection{Greedy:}
\begin{itemize}
    \item BLEU Score: 0.3097
    \item ROUGE-1 Score: 0.3538
    \item ROUGE-2 Score: 0.1297
    \item ROUGE-LCS Score: 0.2704
\end{itemize}

\subsubsection{Random Sampling:}
$\tau = 0.5$
\begin{itemize}
    \item BLEU Score: 0.2863
    \item ROUGE-1 Score: 0.2950
    \item ROUGE-2 Score: 0.1113
    \item ROUGE-LCS Score: 0.2383
\end{itemize}
$\tau = 0.9$
\begin{itemize}
    \item BLEU Score: 0.1996
    \item ROUGE-1 Score: 0.1791
    \item ROUGE-2 Score: 0.0550
    \item ROUGE-LCS Score: 0.1477
\end{itemize}

\subsubsection{Top-k Sampling:}
$k = 5$
\begin{itemize}
    \item BLEU Score: 0.2366
    \item ROUGE-1 Score: 0.2267
    \item ROUGE-2 Score: 0.0607
    \item ROUGE-LCS Score: 0.1738
\end{itemize}
$k = 10$
\begin{itemize}
    \item BLEU Score: 0.2200
    \item ROUGE-1 Score: 0.2204
    \item ROUGE-2 Score: 0.0534
    \item ROUGE-LCS Score: 0.1683
\end{itemize}

\subsubsection{Nucleus Sampling:}
$p = 0.5$
\begin{itemize}
    \item BLEU Score: 0.2825
    \item ROUGE-1 Score: 0.3075
    \item ROUGE-2 Score: 0.0998
    \item ROUGE-LCS Score: 0.2478
\end{itemize}
$p = 0.9$
\begin{itemize}
    \item BLEU Score: 0.1929
    \item ROUGE-1 Score: 0.1922
    \item ROUGE-2 Score: 0.0493
    \item ROUGE-LCS Score: 0.1551
\end{itemize}

\clearpage
\section{Task 1: Word-Constrained Decoding}
We implement strict word-constrained generation using a Trie data structure to enforce valid word formations.

\subsection{Algorithm:}
\subsubsection{Preprocessing:} 
\begin{itemize}
    \item Tokenize each word from the provided word list using the model's tokenizer
    \item Construct a Trie containing all valid token sequences
    \item Create a set of all valid tokens appearing in the word list
\end{itemize}

\subsubsection{Iterative Word-Constrained Decoding:}
\begin{itemize}
    \item Initialize generation with input token IDs and empty sequence state
    \item For each decoding step until maximum length or EOS generation:
    \begin{itemize}
        \item Obtain the model's logits for the next token position
        \item Determine valid next tokens from the Trie:
        \begin{itemize}
            \item During word formation: only permit token continuations that maintain valid prefixes
            \item Between words: only allow tokens that initiate valid words
        \end{itemize}
        \item Apply token constraints by masking invalid options (setting their logits to -inf)
        \item Select the highest probability token from valid candidates
        \item Reset the current word state when a complete word is formed
        \item Terminate immediately if EOS token is generated
    \end{itemize}
    \item Maintain the current token sequence to track word formation progress
    \item Append each generated token to the input for subsequent steps
\end{itemize}

\subsection{BLEU and ROUGE Score:}
\begin{itemize}
    \item BLEU Score: 0.3578
    \item ROUGE-1 Score: 0.4369
    \item ROUGE-2 Score: 0.2502
    \item ROUGE-LCS Score: 0.3770
\end{itemize}

\section{Task 2: Staring into Medusaâ€™s Heads}
We use the Medusa model for word generation, which has multiple logits as outputs for tokens ahead of just the next token, which are obtained from the `heads' of the model.

\subsection{Algorithm}
\subsubsection{Single-Head decoding}
We generate  tokens using single-head generation (greedy decoding). In each iteration, we choose the token with the maximum probability. While this has a high BLEU score,it is  deterministic.

\subsubsection{Multihead decoding: Iterative Beam Search}
We repeatedly generate $s + 1$ tokens at a time, until the number of tokens exceeds the maximum permitted output number of tokens. This forms the output. The first of these is linear, the later ones are from Medusa. We are also given a parameter beam width $w$.

We also apply penalties to EOS tokens so that our sequence does not terminate prematurely. We also apply penalties to sequences involving repetitions of tokens (repeated words). We do this by increasing scores this way:
\begin{itemize}
    \item For EOS tokens, we multiply the (negative) score by an adaptive factor that varies between 10 and 2, with a higher penalty (closer to 10) applied towards the start.
    \item For repeated tokens, we apply a penalty of 5 (multiply score by 5) if the token is same as the previous token or the token 2 steps behind; the factor becomes 3 if it is 3 steps behind. There is no penalty applied for tokens being identical to tokens generated 4 steps behind (or earlier).
\end{itemize}
Any sequence of tokens obtained that ends in an EOS token is identified prematurely, and is appended to a different list that stores such sequences and scores. All other sequences are stored in the normal candidates list, which is updated to store those with the top $w$ scores after each token generation.

In order to avoid multiple model evaluations, we keep one beam search per iteration (do not share all candidates except the best, across iterations).

\subsection{BLEU and ROGUE Score:}
For single-head decoding, the following values were obtained:
\begin{itemize}
    \item BLEU Score: 0.2921
    \item ROUGE-1 Score: 0.3963
    \item ROUGE-2 Score: 0.1483
    \item ROUGE-LCS Score: 0.3177
    \item RTF Score: 0.05526
\end{itemize}

We found the best scores to be with $s = 2$ Medusa heads and beam-width $w = 5$.
\begin{itemize}
    \item BLEU Score: 0.1672
    \item ROUGE-1 Score: 0.2073
    \item ROUGE-2 Score: 0.032
    \item ROUGE-LCS Score: 0.1691
    \item RTF Score: 0.0242
\end{itemize}


\section{Individual Contribution:}
\begin{itemize}
    \item Ojas: LLM Decoding and Word-Constrained Decoding
    \item Muskaan: LLM Decoding, Word-Constrained Decoding and report writing
    \item Tejas: Medusa Model (entirely)
\end{itemize}
\section{References}
\begin{itemize}[noitemsep, nolistsep]
    \item Self-coded, Copilot for keeping the right dimensions, and other syntax. 
    \item Lecture slides for the algorithm
\end{itemize}
\end{document}
